From 37760b9d5f508ebc0cd7e6aa8ea265caee87f149 Mon Sep 17 00:00:00 2001
From: Bharath <linux.bhar@gmail.com>
Date: Thu, 4 Jul 2019 05:29:18 -0600
Subject: [PATCH 4/6] Integrated the predictive memory reclaim logic with
 kswapd. Made optimizations in balance_pgdat to prevent sub-optimal
 performance

---
 mm/vmscan.c | 100 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++----
 1 file changed, 94 insertions(+), 6 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index a815f73..cb02e69 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -3405,6 +3405,67 @@ static void clear_pgdat_congested(pg_data_t *pgdat)
 }
 
 /*
+ * zone_potential_frag checks if there could be a potential fragmentation
+ * event for a particular zone.
+ */
+
+static bool zone_potential_frag(struct zone *zone)
+{
+	unsigned long frag_vec[MAX_ORDER];
+	int order;
+	unsigned long total_free_pages;
+	unsigned long curr_free_pages;
+	bool result;
+
+	total_free_pages = frag_vec[0] = 0;
+	for (order = 0; order < MAX_ORDER; order++) {
+		curr_free_pages = zone->free_area[order].nr_free << order;
+		total_free_pages += curr_free_pages;
+
+		if (order < MAX_ORDER - 1)
+			frag_vec[order + 1] =
+				frag_vec[order] + curr_free_pages;
+	}
+	frag_vec[0] = total_free_pages;
+
+	result = predict(frag_vec, zone->lsq);
+
+	if (result)
+		count_vm_event(POSITIVE_RESULT);
+
+	return result;
+}
+
+/*
+ * node_potential_frag checks if a potential future fragmentation event could
+ * be possible for this node.
+ * It returns true if there is potential fragmentation event for this node.
+ */
+
+static bool node_potential_frag(pg_data_t *pgdat, int classzone_idx)
+{
+	struct zone *zone = NULL;
+	int i, potential_frag = 0;
+
+	for (i = 0; i <= classzone_idx; i++) {
+		zone = pgdat->node_zones + i;
+
+		if (!managed_zone(zone))
+			continue;
+
+		if (zone_potential_frag(zone)) {
+			potential_frag = 1;
+			set_bit(ZONE_POTENTIAL_FRAG, &zone->flags);
+		}
+	}
+
+	if (potential_frag)
+		return true;
+
+	return false;
+}
+
+/*
  * Prepare kswapd for sleeping. This verifies that there are no processes
  * waiting in throttle_direct_reclaim() and that watermarks have been met.
  *
@@ -3432,6 +3493,9 @@ static bool prepare_kswapd_sleep(pg_data_t *pgdat, int order, int classzone_idx)
 	if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
 		return true;
 
+	if (node_potential_frag(pgdat, classzone_idx))
+		return false;
+
 	if (pgdat_balanced(pgdat, order, classzone_idx)) {
 		clear_pgdat_congested(pgdat);
 		return true;
@@ -3505,6 +3569,8 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 	unsigned long nr_boost_reclaim;
 	unsigned long zone_boosts[MAX_NR_ZONES] = { 0, };
 	bool boosted;
+	bool potential_frag = 0;
+	bool need_compact = 0;
 	struct zone *zone;
 	struct scan_control sc = {
 		.gfp_mask = GFP_KERNEL,
@@ -3533,6 +3599,18 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 	}
 	boosted = nr_boost_reclaim;
 
+	for (i = 0; i <= classzone_idx; i++) {
+		zone = pgdat->node_zones + i;
+		if (!managed_zone(zone))
+			continue;
+
+		if (test_bit(ZONE_POTENTIAL_FRAG, &zone->flags)) {
+			potential_frag = 1;
+			need_compact = 1;
+			clear_bit(ZONE_POTENTIAL_FRAG, &zone->flags);
+		}
+	}
+
 restart:
 	sc.priority = DEF_PRIORITY;
 	do {
@@ -3572,8 +3650,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		 * re-evaluate if boosting is required when kswapd next wakes.
 		 */
 		balanced = pgdat_balanced(pgdat, sc.order, classzone_idx);
-		if (!balanced && nr_boost_reclaim) {
+		if (!balanced && (nr_boost_reclaim || potential_frag)) {
 			nr_boost_reclaim = 0;
+			potential_frag = 0;
 			goto restart;
 		}
 
@@ -3582,11 +3661,11 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		 * eligible zones. Note that sc.reclaim_idx is not used as
 		 * buffer_heads_over_limit may have adjusted it.
 		 */
-		if (!nr_boost_reclaim && balanced)
+		if ((!nr_boost_reclaim && !potential_frag) && balanced)
 			goto out;
 
 		/* Limit the priority of boosting to avoid reclaim writeback */
-		if (nr_boost_reclaim && sc.priority == DEF_PRIORITY - 2)
+		if ((nr_boost_reclaim || potential_frag) && sc.priority == DEF_PRIORITY - 2)
 			raise_priority = false;
 
 		/*
@@ -3595,9 +3674,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		 * from reclaim context. If no pages are reclaimed, the
 		 * reclaim will be aborted.
 		 */
-		sc.may_writepage = !laptop_mode && !nr_boost_reclaim;
-		sc.may_swap = !nr_boost_reclaim;
-		sc.may_shrinkslab = !nr_boost_reclaim;
+		sc.may_writepage = !laptop_mode && !nr_boost_reclaim && !potential_frag;
+		sc.may_swap = !nr_boost_reclaim && !potential_frag;
+		sc.may_shrinkslab = !nr_boost_reclaim && !potential_frag;
 
 		/*
 		 * Do some background aging of the anon list, to give
@@ -3660,6 +3739,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		if (nr_boost_reclaim && !nr_reclaimed)
 			break;
 
+		if (potential_frag && !nr_reclaimed)
+			break;
+
 		if (raise_priority || !nr_reclaimed)
 			sc.priority--;
 	} while (sc.priority >= 1);
@@ -3690,9 +3772,15 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		wakeup_kcompactd(pgdat, pageblock_order, classzone_idx);
 	}
 
+	if (need_compact) {
+		count_vm_event(KCOMPACTD_BY_PRED_MEM);
+		wakeup_kcompactd(pgdat, pageblock_order, classzone_idx);
+	}
+
 	snapshot_refaults(NULL, pgdat);
 	__fs_reclaim_release();
 	psi_memstall_leave(&pflags);
+
 	/*
 	 * Return the order kswapd stopped reclaiming at as
 	 * prepare_kswapd_sleep() takes it into account. If another caller
-- 
1.8.3.1

