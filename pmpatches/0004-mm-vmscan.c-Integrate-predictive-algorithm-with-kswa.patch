From 30dfc5885b480d245f8b824ef44038d4eb5a3397 Mon Sep 17 00:00:00 2001
From: Bharath <linux.bhar@gmail.com>
Date: Mon, 8 Jul 2019 06:16:20 -0600
Subject: [PATCH 4/4] mm/vmscan.c: Integrate predictive algorithm with kswapd

This patch integrates the previously talked about predictive algorithm
with kswapd.

We introduce 2 new functions:
(i) node_potential_frag: This functions takes in the pgdat pointer and classzone_idx.
It iterates through all the zones and sets the ZONE_POTENTIAL_FRAG flag for all the zones
that could have a potential fragmentation event. It has to iterate through all the zones
even if one zone returns true as we need to update the lsq_structs of all the zones

(ii) zone_potential_frag: This function uses the info from zone->free_area and calulates the
fragmented free memory at a given order which is the sum of occupancies at the lower order.

node_potential_frag is called in prepare_kswapd_sleep to check if the node could have
a potential fragmentation event which will not let kswapd sleep. This has a higher priority than
the check for watermarks as a zone could be balanced but could be fragmented.

In balance_pgdat, we kind of unify watermark_boosting and the external fragmentation 
detection algorithm. This is because the intent of both algorithms is the same, to reduce
external fragmentation by reclaiming a small amount of memory and then waking up kcompactd.

This "unification" is important as there a bunch of optimizations done during such balance_pgdat
runs like preventing suboptimal I/O and not making kswapd work too hard(by limiting the reclaim
priority to DEF_PRIORITY - 2).

Signed-off-by: Bharath <linux.bhar@gmail.com>
---
 mm/vmscan.c | 150 +++++++++++++++++++++++++++++++++++++++++++++++++++---------
 1 file changed, 128 insertions(+), 22 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index a815f73..0220137 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -3405,8 +3405,71 @@ static void clear_pgdat_congested(pg_data_t *pgdat)
 }
 
 /*
+ * zone_potential_frag checks if there could be a potential fragmentation
+ * event for a particular zone.
+ */
+
+static bool zone_potential_frag(struct zone *zone)
+{
+	unsigned long frag_vec[MAX_ORDER];
+	int order;
+	unsigned long total_free_pages;
+	unsigned long curr_free_pages;
+	bool result;
+
+	total_free_pages = frag_vec[0] = 0;
+	for (order = 0; order < MAX_ORDER; order++) {
+		curr_free_pages = zone->free_area[order].nr_free << order;
+		total_free_pages += curr_free_pages;
+
+		if (order < MAX_ORDER - 1)
+			frag_vec[order + 1] =
+				frag_vec[order] + curr_free_pages;
+	}
+	frag_vec[0] = total_free_pages;
+
+	result = predict(frag_vec, zone->lsq);
+
+	if (result)
+		count_vm_event(POSITIVE_RESULT);
+
+	return result;
+}
+
+/*
+ * node_potential_frag checks if a potential future fragmentation event could
+ * be possible for this node.
+ * It returns true if there is potential fragmentation event for this node.
+ */
+
+static bool node_potential_frag(pg_data_t *pgdat, int classzone_idx)
+{
+	struct zone *zone = NULL;
+	int i, potential_frag = 0;
+
+	for (i = 0; i <= classzone_idx; i++) {
+		zone = pgdat->node_zones + i;
+
+		if (!managed_zone(zone))
+			continue;
+
+		if (zone_potential_frag(zone)) {
+			potential_frag = 1;
+			set_bit(ZONE_POTENTIAL_FRAG, &zone->flags);
+		}
+	}
+
+	if (potential_frag)
+		return true;
+
+	return false;
+}
+
+/*
  * Prepare kswapd for sleeping. This verifies that there are no processes
  * waiting in throttle_direct_reclaim() and that watermarks have been met.
+ * It also checks if this node could have a potential external fragmentation
+ * event which could lead to direct reclaim/compaction stalls.
  *
  * Returns true if kswapd is ready to sleep
  */
@@ -3431,6 +3494,15 @@ static bool prepare_kswapd_sleep(pg_data_t *pgdat, int order, int classzone_idx)
 	/* Hopeless node, leave it to direct reclaim */
 	if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
 		return true;
+	/*
+	 * Check whether this node could have a potential external
+	 * fragmentation event. If we detect a potential external
+	 * fragmentation event, we don't allow kswapd to sleep and
+	 * perform compaction at the end of a balance_pgdat run.
+	 */
+
+	if (node_potential_frag(pgdat, classzone_idx))
+		return false;
 
 	if (pgdat_balanced(pgdat, order, classzone_idx)) {
 		clear_pgdat_congested(pgdat);
@@ -3505,6 +3577,8 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 	unsigned long nr_boost_reclaim;
 	unsigned long zone_boosts[MAX_NR_ZONES] = { 0, };
 	bool boosted;
+	bool potential_frag = 0;
+	bool need_compact;
 	struct zone *zone;
 	struct scan_control sc = {
 		.gfp_mask = GFP_KERNEL,
@@ -3530,9 +3604,25 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 
 		nr_boost_reclaim += zone->watermark_boost;
 		zone_boosts[i] = zone->watermark_boost;
+
+		/*
+		 * Check if any of the zones could have a potential fragmentation event.
+		 */
+		if (test_bit(ZONE_POTENTIAL_FRAG, &zone->flags)) {
+			potential_frag = 1;
+			clear_bit(ZONE_POTENTIAL_FRAG, &zone->flags);
+		}
 	}
 	boosted = nr_boost_reclaim;
 
+	/*
+	 * If kswapd is woken up because of watermark boosting or forced to run
+	 * another balance_pgdat run because it detected an external fragmentation event,
+	 * we need to compact after reclaiming some pages. need_compact is true if such compaction
+	 * is required.
+	 */
+	need_compact = boosted || potential_frag;
+
 restart:
 	sc.priority = DEF_PRIORITY;
 	do {
@@ -3540,6 +3630,17 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		bool raise_priority = true;
 		bool balanced;
 		bool ret;
+		/*
+		 * Kswapd can be called in 2 ways. One way is by exhausting the number of pages
+		 * below the watermarks. The other way is to reduce the amount of fragmentation.
+		 * Kswapd can be called when watermarks are boosted or be forced to run
+		 * balance_pgdat again when a zone is detected with a potential external
+		 * fragmentation event. If kswapd is running due to watermark boosting
+		 * or when an external fragmentation event is detected, we don't want to
+		 * perform any suboptimal I/O or make kswapd work too hard. This is
+		 * important from a performance point of view.
+		 */
+		bool defrag_run = nr_boost_reclaim || potential_frag;
 
 		sc.reclaim_idx = classzone_idx;
 
@@ -3565,39 +3666,41 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		}
 
 		/*
-		 * If the pgdat is imbalanced then ignore boosting and preserve
-		 * the watermarks for a later time and restart. Note that the
-		 * zone watermarks will be still reset at the end of balancing
-		 * on the grounds that the normal reclaim should be enough to
-		 * re-evaluate if boosting is required when kswapd next wakes.
+		 * If the pgdat is imbalanced then ignore the deframentatio run. In
+		 * the case of watermark boosting, preserve the watermarks for
+		 * a later time and restart. Note that the zone watermarks will
+		 * be still reset at the end of balancing on the grounds that the
+		 * normal reclaim should be enough to re-evaluate if boosting is
+		 * required when kswapd next wakes.
 		 */
 		balanced = pgdat_balanced(pgdat, sc.order, classzone_idx);
-		if (!balanced && nr_boost_reclaim) {
+		if (!balanced && defrag_run) {
 			nr_boost_reclaim = 0;
+			potential_frag = 0;
 			goto restart;
 		}
 
 		/*
-		 * If boosting is not active then only reclaim if there are no
+		 * If this is not a defrag run then only reclaim if there are no
 		 * eligible zones. Note that sc.reclaim_idx is not used as
 		 * buffer_heads_over_limit may have adjusted it.
 		 */
-		if (!nr_boost_reclaim && balanced)
+		if (!defrag_run && balanced)
 			goto out;
 
-		/* Limit the priority of boosting to avoid reclaim writeback */
-		if (nr_boost_reclaim && sc.priority == DEF_PRIORITY - 2)
+		/* Limit the priority of defrag_runs to avoid reclaim writeback */
+		if (defrag_run && sc.priority == DEF_PRIORITY - 2)
 			raise_priority = false;
 
 		/*
-		 * Do not writeback or swap pages for boosted reclaim. The
+		 * Do not writeback or swap pages for defragmentation runs. The
 		 * intent is to relieve pressure not issue sub-optimal IO
 		 * from reclaim context. If no pages are reclaimed, the
 		 * reclaim will be aborted.
 		 */
-		sc.may_writepage = !laptop_mode && !nr_boost_reclaim;
-		sc.may_swap = !nr_boost_reclaim;
-		sc.may_shrinkslab = !nr_boost_reclaim;
+		sc.may_writepage = !laptop_mode && !defrag_run;
+		sc.may_swap = !defrag_run;
+		sc.may_shrinkslab = !defrag_run;
 
 		/*
 		 * Do some background aging of the anon list, to give
@@ -3651,13 +3754,13 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		 */
 		nr_reclaimed = sc.nr_reclaimed - nr_reclaimed;
 		nr_boost_reclaim -= min(nr_boost_reclaim, nr_reclaimed);
-
+		defrag_run = nr_boost_reclaim || potential_frag;
 		/*
-		 * If reclaim made no progress for a boost, stop reclaim as
+		 * If reclaim made no progress for a defragmentation run, stop reclaim as
 		 * IO cannot be queued and it could be an infinite loop in
 		 * extreme circumstances.
 		 */
-		if (nr_boost_reclaim && !nr_reclaimed)
+		if (defrag_run && !nr_reclaimed)
 			break;
 
 		if (raise_priority || !nr_reclaimed)
@@ -3682,17 +3785,20 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 			zone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);
 			spin_unlock_irqrestore(&zone->lock, flags);
 		}
-
-		/*
-		 * As there is now likely space, wakeup kcompact to defragment
-		 * pageblocks.
-		 */
+	}
+
+	/*
+	 * As there is now likely space, wakeup kcompactd to defragment pageblocks.
+	 */
+	if (need_compact) {
+		count_vm_event(KCOMPACTD_BY_PRED_MEM);
 		wakeup_kcompactd(pgdat, pageblock_order, classzone_idx);
 	}
 
 	snapshot_refaults(NULL, pgdat);
 	__fs_reclaim_release();
 	psi_memstall_leave(&pflags);
+
 	/*
 	 * Return the order kswapd stopped reclaiming at as
 	 * prepare_kswapd_sleep() takes it into account. If another caller
-- 
1.8.3.1

