From 66153ca4389729b4159953d995eb76f12933eff9 Mon Sep 17 00:00:00 2001
From: Bharath Vedartham <linux.bhar@gmail.com>
Date: Tue, 6 Aug 2019 12:36:46 -0600
Subject: [PATCH 2/2] mm/vmscan: Add fragmentation prediction to kswapd

This patch adds proactive memory reclamation to kswapd using the
free page exhaustion/fragmentation prediction based upon memory
consumption trend. It uses the least squares fit algorith introduced
earlier for this prediction. A new function node_potential_frag()
iterates through all zones and updates trend data in the lookback
window for least square fit algorithm. At the same time it flags any
zones that have potential for exhaustion/fragmentation by setting
ZONE_POTENTIAL_FRAG flag.

prepare_kswapd_sleep() calls node_potential_frag() to check if the
node has potential exhaustion/fragmentation. If so, kswapd will
continue reclamataion.

The return value from node_potential_frag is used to determine whether compaction should be done or memory reclaim.

balance_pgdat is optimized for cases when kswapd executes
balance_pgdat because of a MEMPREDICT_RECLAIM. In
such runs, suboptimal I/O is not performed and kswapd is not worked
too hard. These optimizations have been borrowed from Mel Gorman.

Signed-off-by: Bharath Vedartham <linux.bhar@gmail.com>
---
 mm/vmscan.c | 171 +++++++++++++++++++++++++++++++++++++++++++++++++++---------
 1 file changed, 147 insertions(+), 24 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index dbdc46a..b36ac7d 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -51,6 +51,7 @@
 #include <linux/printk.h>
 #include <linux/dax.h>
 #include <linux/psi.h>
+#include <linux/jiffies.h>
 
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -3405,13 +3406,83 @@ static void clear_pgdat_congested(pg_data_t *pgdat)
 }
 
 /*
+ * Check if there could be a potential fragmentation event for a
+ * particular zone.
+ */
+
+static int zone_potential_frag(struct zone *zone)
+{
+	struct frag_info frag_vec[MAX_ORDER];
+	int order;
+	unsigned long total_free_pages;
+	unsigned long curr_free_pages;
+	bool result;
+
+	total_free_pages = frag_vec[0].free_pages = 0;
+	for (order = 0; order < MAX_ORDER; order++) {
+		curr_free_pages = zone->free_area[order].nr_free << order;
+		total_free_pages += curr_free_pages;
+
+		if (order < MAX_ORDER - 1) {
+			frag_vec[order + 1].free_pages =
+				frag_vec[order].free_pages + curr_free_pages;
+			frag_vec[order + 1].time =
+				jiffies64_to_msecs(get_jiffies_64()
+				- INITIAL_JIFFIES);
+		}
+	}
+	frag_vec[0].free_pages = total_free_pages;
+	frag_vec[0].time = frag_vec[MAX_ORDER - 1].time;
+
+	result = mem_predict(frag_vec, zone->mem_prediction, zone);
+
+	return result;
+}
+
+/*
+ * Check if a potential future fragmentation event could be possible for
+ * this node.
+ * It returns true if there is potential fragmentation event for this node.
+ */
+
+static bool node_potential_frag(pg_data_t *pgdat, int classzone_idx)
+{
+	struct zone *zone = NULL;
+	int i, potential_frag = 0, retval = 0;
+
+	for (i = 0; i <= classzone_idx; i++) {
+		zone = pgdat->node_zones + i;
+
+		if (!managed_zone(zone))
+			continue;
+
+		retval |= zone_potential_frag(zone);
+		/*
+		 * We are going to perform another run of 
+		 * balance_pgdat if MEMPREDICT_RECLAIM is true,
+		 * the zone flag is used for I/O related optimizations.
+		 */
+		if (retval & MEMPREDICT_RECLAIM) {
+			potential_frag = 1;
+			set_bit(ZONE_POTENTIAL_FRAG, &zone->flags);
+		}
+	}
+
+	return retval;
+}
+
+/*
  * Prepare kswapd for sleeping. This verifies that there are no processes
  * waiting in throttle_direct_reclaim() and that watermarks have been met.
+ * It also checks if this node could have a potential external fragmentation
+ * event which could lead to direct reclaim/compaction stalls.
  *
  * Returns true if kswapd is ready to sleep
  */
 static bool prepare_kswapd_sleep(pg_data_t *pgdat, int order, int classzone_idx)
 {
+	int retval = 0;
+	
 	/*
 	 * The throttled processes are normally woken up in balance_pgdat() as
 	 * soon as allow_direct_reclaim() is true. But there is a potential
@@ -3431,6 +3502,19 @@ static bool prepare_kswapd_sleep(pg_data_t *pgdat, int order, int classzone_idx)
 	/* Hopeless node, leave it to direct reclaim */
 	if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
 		return true;
+	/*
+	 * Check whether this node could have a potential 
+	 * memory exhaustion event. If the exhaustion involves reclaim
+	 * then don't let kswapd sleep. Otherwise 
+	 */
+	retval = node_potential_frag(pgdat, classzone_idx);
+	if ((retval & MEMPREDICT_RECLAIM) && 
+			(retval & MEMPREDICT_COMPACT))
+		return false;
+	if (retval & MEMPREDICT_RECLAIM)
+		return false;
+	if (retval & MEMPREDICT_COMPACT)
+		return true;
 
 	if (pgdat_balanced(pgdat, order, classzone_idx)) {
 		clear_pgdat_congested(pgdat);
@@ -3505,6 +3589,8 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 	unsigned long nr_boost_reclaim;
 	unsigned long zone_boosts[MAX_NR_ZONES] = { 0, };
 	bool boosted;
+	bool potential_frag = 0;
+	bool need_compact;
 	struct zone *zone;
 	struct scan_control sc = {
 		.gfp_mask = GFP_KERNEL,
@@ -3531,9 +3617,27 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 
 		nr_boost_reclaim += zone->watermark_boost;
 		zone_boosts[i] = zone->watermark_boost;
+
+		/*
+		 * Check if any of the zones could have a potential
+		 * fragmentation event.
+		 */
+		if (test_bit(ZONE_POTENTIAL_FRAG, &zone->flags)) {
+			potential_frag = 1;
+			clear_bit(ZONE_POTENTIAL_FRAG, &zone->flags);
+		}
 	}
 	boosted = nr_boost_reclaim;
 
+	/*
+	 * If kswapd is woken up because of watermark boosting or forced
+	 * to run another balance_pgdat run because it detected an
+	 * external fragmentation event, we need to compact after
+	 * reclaiming some pages. need_compact is true if such compaction
+	 * is required.
+	 */
+	need_compact = boosted || potential_frag;
+
 restart:
 	sc.priority = DEF_PRIORITY;
 	do {
@@ -3541,6 +3645,19 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		bool raise_priority = true;
 		bool balanced;
 		bool ret;
+		/*
+		 * Kswapd can be called because the number of free pages
+		 * has fallen below the watermark. Another reason it could
+		 * be called is to reduce the amount of fragmentation.
+		 * Kswapd can be called when watermarks are boosted or be
+		 * forced to run balance_pgdat again when a zone has
+		 * potential external fragmentation event. If kswapd is
+		 * running due to watermark boosting or when an external
+		 * fragmentation event is detected, we don't want to
+		 * perform any suboptimal I/O or make kswapd work too hard.
+		 * This is important from a performance point of view.
+		 */
+		bool defrag_run = nr_boost_reclaim || potential_frag;
 
 		sc.reclaim_idx = classzone_idx;
 
@@ -3566,39 +3683,44 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		}
 
 		/*
-		 * If the pgdat is imbalanced then ignore boosting and preserve
-		 * the watermarks for a later time and restart. Note that the
-		 * zone watermarks will be still reset at the end of balancing
-		 * on the grounds that the normal reclaim should be enough to
-		 * re-evaluate if boosting is required when kswapd next wakes.
+		 * If the pgdat is imbalanced then ignore the deframentatio run. In
+		 * the case of watermark boosting, preserve the watermarks for
+		 * a later time and restart. Note that the zone watermarks will
+		 * be still reset at the end of balancing on the grounds that the
+		 * normal reclaim should be enough to re-evaluate if boosting is
+		 * required when kswapd next wakes.
 		 */
 		balanced = pgdat_balanced(pgdat, sc.order, classzone_idx);
-		if (!balanced && nr_boost_reclaim) {
+		if (!balanced && defrag_run) {
 			nr_boost_reclaim = 0;
+			potential_frag = 0;
 			goto restart;
 		}
 
 		/*
-		 * If boosting is not active then only reclaim if there are no
+		 * If this is not a defrag run then only reclaim if there are no
 		 * eligible zones. Note that sc.reclaim_idx is not used as
 		 * buffer_heads_over_limit may have adjusted it.
 		 */
-		if (!nr_boost_reclaim && balanced)
+		if (!defrag_run && balanced)
 			goto out;
 
-		/* Limit the priority of boosting to avoid reclaim writeback */
-		if (nr_boost_reclaim && sc.priority == DEF_PRIORITY - 2)
+		/*
+		 * Limit the priority of defrag_runs to avoid reclaim
+		 * writeback
+		 */
+		if (defrag_run && sc.priority == DEF_PRIORITY - 2)
 			raise_priority = false;
 
 		/*
-		 * Do not writeback or swap pages for boosted reclaim. The
+		 * Do not writeback or swap pages for defragmentation runs. The
 		 * intent is to relieve pressure not issue sub-optimal IO
 		 * from reclaim context. If no pages are reclaimed, the
 		 * reclaim will be aborted.
 		 */
-		sc.may_writepage = !laptop_mode && !nr_boost_reclaim;
-		sc.may_swap = !nr_boost_reclaim;
-		sc.may_shrinkslab = !nr_boost_reclaim;
+		sc.may_writepage = !laptop_mode && !defrag_run;
+		sc.may_swap = !defrag_run;
+		sc.may_shrinkslab = !defrag_run;
 
 		/*
 		 * Do some background aging of the anon list, to give
@@ -3652,13 +3774,13 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		 */
 		nr_reclaimed = sc.nr_reclaimed - nr_reclaimed;
 		nr_boost_reclaim -= min(nr_boost_reclaim, nr_reclaimed);
-
+		defrag_run = nr_boost_reclaim || potential_frag;
 		/*
-		 * If reclaim made no progress for a boost, stop reclaim as
-		 * IO cannot be queued and it could be an infinite loop in
-		 * extreme circumstances.
+		 * If reclaim made no progress for a defragmentation run,
+		 * stop reclaim as IO cannot be queued and it could be an
+		 * infinite loop in extreme circumstances.
 		 */
-		if (nr_boost_reclaim && !nr_reclaimed)
+		if (defrag_run && !nr_reclaimed)
 			break;
 
 		if (raise_priority || !nr_reclaimed)
@@ -3683,13 +3805,14 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 			zone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);
 			spin_unlock_irqrestore(&zone->lock, flags);
 		}
+	}
 
-		/*
-		 * As there is now likely space, wakeup kcompact to defragment
-		 * pageblocks.
-		 */
+	/*
+	 * As there is now likely space, wakeup kcompactd to defragment
+	 * pageblocks.
+	 */
+	if (need_compact)
 		wakeup_kcompactd(pgdat, pageblock_order, classzone_idx);
-	}
 
 	snapshot_refaults(NULL, pgdat);
 	__fs_reclaim_release();
-- 
1.8.3.1

