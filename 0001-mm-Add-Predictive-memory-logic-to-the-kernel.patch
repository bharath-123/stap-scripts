From 4f5a476956b5a13e3d330b1347a96e678dc4e2e2 Mon Sep 17 00:00:00 2001
From: Bharath <linux.bhar@gmail.com>
Date: Mon, 8 Jul 2019 05:55:07 -0600
Subject: [PATCH 1/4] mm: Add Predictive memory logic to the kernel

This patch adds a new file mm/lsq.c which contains the logic for the predictive memory
reclaim.

A new structure lsq_struct is added to the zone structure. The lsq_struct is a per-zone
and per-order structure which contains information like the current value of sigma(y) and
sigma(xy) which are used to calculate the least square lines.
LOOKBACK is the number of steps to lookback to calculate the trends in free memory consumption
and memory fragmentation rate.

A new zone flag is added, ZONE_POTENTIAL_FRAG. This flag is set when the zone is detected
with a potential external fragmentation event.

Signed-off-by: Bharath <linux.bhar@gmail.com>
---
 include/linux/mmzone.h |  26 ++++++++
 mm/lsq.c               | 168 +++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 194 insertions(+)
 create mode 100644 mm/lsq.c

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fba7741..a4a47a8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -302,6 +302,26 @@ struct per_cpu_nodestat {
 
 #endif /* !__GENERATING_BOUNDS.H */
 
+/*
+ * The is the number of steps to lookback to in the fragmentation exhaustion prediction
+ * algorithm.
+ */
+#define LOOKBACK 8
+
+/*
+ * This is a per zone structure which contains the details to compute a least square line.
+ * There is one structure for each order. 
+ */
+struct lsq_struct {
+	int slot;
+	int ready;
+	long long y[LOOKBACK];
+	long long sum_xy;
+	long long sum_y;
+};
+
+extern int predict(unsigned long *frag_vec, struct lsq_struct *lsq);
+
 enum zone_type {
 #ifdef CONFIG_ZONE_DMA
 	/*
@@ -502,6 +522,9 @@ struct zone {
 
 	bool			contiguous;
 
+	/* lsq structures for each order */
+	struct lsq_struct	lsq[MAX_ORDER];
+
 	ZONE_PADDING(_pad3_)
 	/* Zone statistics */
 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
@@ -526,6 +549,9 @@ enum zone_flags {
 	ZONE_BOOSTED_WATERMARK,		/* zone recently boosted watermarks.
 					 * Cleared when kswapd is woken.
 					 */
+	ZONE_POTENTIAL_FRAG,		/* zone detected with a potential external fragmentation
+					 * event. 
+				         */
 };
 
 static inline unsigned long zone_managed_pages(struct zone *zone)
diff --git a/mm/lsq.c b/mm/lsq.c
new file mode 100644
index 0000000..49438b0
--- /dev/null
+++ b/mm/lsq.c
@@ -0,0 +1,168 @@
+/*
+ * Least squares implementation
+ */
+
+#include <linux/mmzone.h>
+#include <linux/math64.h>
+/*
+ * This is the compaction rate expressed in pages/s. Right now we take a 
+ * default value of 2500
+ */
+unsigned long R_c = 2500;
+/*
+ * This is the fragmentation threshold. This is the amount of pages which can be
+ * lost due to fragmentation.
+ */
+unsigned long threshold = 0;
+
+/*
+ * SUM_X and SUM_XX is sigma(x) and sigma(x^2) for natural numbers from [1, LOOKBACK].
+ * This is used to calculate the least square fit line.
+ */
+long long SUM_X = ((LOOKBACK * (1 - LOOKBACK)) / 2);
+long long SUM_XX = ((LOOKBACK * (LOOKBACK - 1) * (2 * LOOKBACK - 1)) / 6);
+
+/*
+ * This function inserts the given value into the list of most recently seen
+ * data and returns the parameters, m and c, of a straight line of the form
+ * y = mx + c that, according to the the method of least squares, fits them
+ * best.  The formulation is for the special case in which x_i = i + 1 - N;
+ * this reduces the need for storage and permits constant time updates.
+ */
+static int
+lsq_fit(struct lsq_struct *lsq, unsigned long new_y, long long *m, long long *c)
+{
+	unsigned long long oldest_y;
+
+	oldest_y = lsq->y[lsq->slot];
+
+	lsq->sum_xy -= lsq->sum_y;
+	if (lsq->ready)
+		lsq->sum_xy += LOOKBACK * oldest_y;
+
+	lsq->sum_y += new_y;
+	if (lsq->ready)
+		lsq->sum_y -= oldest_y;
+
+	lsq->y[lsq->slot++] = new_y;
+
+	if (lsq->slot == LOOKBACK) {
+		lsq->slot = 0;
+		lsq->ready = 1;
+	}
+
+	if (!lsq->ready)
+		return -1;
+
+	*m = div64_long((LOOKBACK * lsq->sum_xy - SUM_X * lsq->sum_y),
+			(LOOKBACK * SUM_XX - SUM_X * SUM_X));
+
+	*c = div64_long((lsq->sum_y - *m * SUM_X), LOOKBACK);
+
+	return 0;
+}
+
+/*
+ * This function determines whether it is necessary to begin compaction now in
+ * order to avert exhaustion of any of the free lists.  Its basis is a simple
+ * model in which the total free memory, f_T, is consumed at a constant rate,
+ * R_T, i.e.
+ *      
+ *	f_T(t) = R_T t + f_T(0)
+ *        
+ * For any given order, o, members of subordinate lists constitute fragmented
+ * free memory, f_f(o):  the blocks are notionally free but they are unavailable
+ * for allocation.  The fragmented free memory is also assumed to behave
+ * linearly and in the absence of compaction is given by
+ *             
+ *	f_f(o, t) = R_f(o) t + f_f(o, 0)
+ *               
+ * Compaction is assumed to proceed at a constant rate, R_c, that is independent
+ * of order.
+ *                  
+ * It is assumed that all allocations will be made from contiguous memory
+ * meaning that, under net memory pressure and with no change in fragmentation,
+ * f_T will become equal to f_f and subsequent allocations will stall in either
+ * direct compaction or reclaim.  Preemptive compaction will delay the onset of
+ * exhaustion but, to be useful, must begin early enough and must proceed at a
+ * sufficient rate.
+ *                         
+ * On each invocation, this function obtains estimates for the parameters
+ * f_T(0), R_T, f_f(o, 0) and R_F(o).  It determines whether, if R_T and R_f(o)
+ * remain constant and compaction begins at the next invocation, f_T(t) and
+ * f_f(o, t) will intersect in the future at a point corresponding to a level of
+ * free memory above some pre-defined limit that represents the lowest
+ * achievable degree of fragmentation.  If this is the case then the function
+ * indicates that compaction should begin now by returning a pointer to the same
+ * structure passed in, now populated with parameters describing the anticipated
+ * exhaustion.  The function returns NULL if no action is needed before the
+ * start of the next time interval.                              
+ */
+int predict(unsigned long *frag_vec, struct lsq_struct *lsq)
+{
+	int order;
+	long long m[MAX_ORDER];
+	long long c[MAX_ORDER];
+	int is_ready = 1;
+	long long f_T_zero, R_T, f_f_zero, R_f, t_e, f_e;
+
+	for(order = 0; order < MAX_ORDER; order++) {
+		if (lsq_fit(&lsq[order], frag_vec[order], &m[order],
+				&c[order]) == -1)
+			is_ready = 0;
+	}
+
+	if (!is_ready)
+		return 0;
+
+	f_T_zero = c[0];
+	R_T = m[0];
+
+	for (order = 1; order < MAX_ORDER; order++) {
+		f_f_zero = c[order];
+		R_f = m[order];
+
+		if (f_T_zero <= f_f_zero)
+			continue;
+		/*
+		 * There are only two reasons to begin compaction immediately,
+		 * i.e. at the beginning of this interval.  The first is that
+		 * the alternative would be exhaustion before the beginning of
+		 * the next interval.
+		 */
+		if (R_T < R_f) {
+			t_e = div64_long((f_T_zero - f_f_zero),
+					(R_f - R_T));
+			if (t_e < 1) {
+				/*
+				 * Don't bother compacting if the expected
+				 * fragmentation improves upon the given
+				 * threshold.																	*/
+				f_e = div64_long(((f_T_zero - f_f_zero) * R_T),
+						((R_f - R_T) + f_T_zero));
+
+				if (f_e > threshold)
+					return 1;
+			}
+		}
+
+		/*
+		 * The second reason is that deferring compaction until the
+		 * start of the next interval would result, at the time of
+		 * exhaustion, in a surfeit of free fragmented memory above the
+		 * desired threshold.
+		 */
+		if (R_T < R_f + R_c) {
+			t_e = div64_long((f_T_zero - f_f_zero + R_c),
+					(R_f + R_c - R_T));
+			if (t_e > 1) {
+				f_e = div64_long(((f_T_zero - f_f_zero + R_c) * R_T) ,
+						((R_f + R_c - R_T) + f_T_zero));
+				if (f_e > threshold)
+					return 1;
+			}
+		}
+	}
+
+	return 0;
+}
-- 
1.8.3.1

